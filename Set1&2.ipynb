{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# initialize a new Spark Context to use for the execution of the script\n",
    "sc = SparkContext(appName=\"MY-APP-NAME\", master=\"local[*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Given a list of numbers separate odd numbers from even numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE \n",
    "# Create a file with the following list of class,member pairs:\n",
    "# 1,2;1,2;1,3;1,4;1,5;2,1;2,4;2,4;2,7;2,9;3,1;3,2;3,3;3,6;3,1\n",
    "# Read the file and write the code to obtain the list of elements for each class\n",
    "\n",
    "text_rdd = sc.textFile(\"ex1.txt\")\n",
    "pairs_rdd = text_rdd.flatMap(lambda x: x.split(\";\")).map(lambda x: x.split(\",\")).reduceByKey(lambda x,y: x+\",\"+y)\n",
    "\n",
    "print (pairs_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #EXERCISE\n",
    "# Read the txt file with the log of songs an user listened \n",
    "# to last week and find their top three favourite songs,\n",
    "# the least favourite song and plot the distribution of\n",
    "# the number of listens \n",
    "\n",
    "log_of_songs_rdd = sc.parallelize(log_of_songs)\n",
    "playCounts = log_of_songs_rdd\\\n",
    "                .map(lambda x: (x, 1))\\\n",
    "                .reduceByKey(lambda x, y : x+y)\n",
    "topSongs = playCounts.takeOrdered(3, (lambda t: -t[1]))\n",
    "topSongs = [t[0] for t in topSongs]\n",
    "leastFav = playCounts.takeOrdered(1, (lambda t: t[1]))[0]\n",
    "print(topSongs)\n",
    "print(f\"The user's last week's top three songs are {topSongs[0]}, {topSongs[1]} and {topSongs[2]}\")\n",
    "print(f\"The user's last week's least favourite song is {leastFav}\")\n",
    "\n",
    "orderedCounts = playCounts.takeOrdered(len(playCounts.collect()), (lambda t: -t[1]))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(\n",
    "    x=[l for l, freq in orderedCounts],\n",
    "    height=[freq for l, freq in orderedCounts]\n",
    ")\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Generate 1000 random points (x,y) and 20 seed points(a,b). \n",
    "# Assign the points to the closer seed and than \n",
    "# count how many points are assigned to each seed.\n",
    "\n",
    "from random import random\n",
    "import numpy as np\n",
    "\n",
    "def closer(elem,listS):\n",
    "    min = 1000000\n",
    "    min_i = -1\n",
    "    for e in listS:\n",
    "        elem=np.array(elem)\n",
    "        seed=np.array(e[1])\n",
    "        p=np.sum((elem-seed)**2)\n",
    "        d=np.sqrt(p)\n",
    "        if min>d: \n",
    "            min = d\n",
    "            min_i = e[0]\n",
    "    return min_i\n",
    "\n",
    "points = []\n",
    "for _ in range(1000):\n",
    "    value_x = random()\n",
    "    value_y = random()\n",
    "    points.append((value_x,value_y))\n",
    "    \n",
    "seeds = []\n",
    "for i in range(20):\n",
    "    value_x = random()\n",
    "    value_y = random()\n",
    "    seeds.append((i,(value_x,value_y)))\n",
    "    \n",
    "points_rdd = sc.parallelize(points)\n",
    "distances_rdd = points_rdd.map(lambda x: closer(x, seeds)).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).sortByKey()\n",
    "\n",
    "print(distances_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Compute the numer of words appearing in both commedies and tragedies and the number of words which are not \n",
    "# present in both the texts\n",
    "\n",
    "text1 = sc.textFile(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa00/comedies.txt\")\n",
    "text2 = sc.textFile(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa00/tragedies.txt\")\n",
    "words1 = text1.flatMap(lambda x: x.split())\n",
    "words2 = text2.flatMap(lambda x: x.split())\n",
    "both = words1.intersection(words2)\n",
    "\n",
    "print(both.count())\n",
    "print(words1.count()+words2.count()-both.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Read the file commedies and count the number of rows containing the word \"skill\"\n",
    "\n",
    "text = sc.textFile(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa00/comm2.txt\")\n",
    "pair_rdd = text.map(lambda x: x.lower().split()).filter(lambda x: \"skill\" in x)\n",
    "print (pair_rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Read the file commedies and count the number of words having the same second letter.\n",
    "\n",
    "text = sc.textFile(\"hdfs://kddrtserver11.isti.cnr.it:9000/user/hpsa00/comm2.txt\")\n",
    "pair_rdd = text.flatMap(lambda x: x.split()).filter(lambda x: len(x)>2).map(lambda x: (x[1].upper(), 1)).reduceByKey(lambda x,y:x+y)\n",
    "print (pair_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_rdd = sc.textFile(\"hdfs://masterbig-1.itc.unipi.it:54310/masterbig_data/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'txt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21824/532564856.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Remove stop-words from text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'and'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'the'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'is'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfiltered_txt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mfiltered_txt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'txt' is not defined"
     ]
    }
   ],
   "source": [
    "#EXERCISE\n",
    "#Clean text\n",
    "pattern = re.compile(\"[^a-z0-9 ]+\")\n",
    "shakespeare_clean_rdd = shakespeare_rdd.map(lambda line: pattern.sub(' ', line.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE \n",
    "# Compute the number of distinct words\n",
    "shakespeare_clean.flatMap(lambda line: line.split()).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Remove punctuation. Often punctuation is just noise, and it is here. Do a Map and/or Filter (some punctuation is\n",
    "# attached to words, and some is not) to eliminate all punctuation from our Shakespeare data. Note that if you are\n",
    "# familiar with regular expressions, Python has a ready method to use those.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXERCISE\n",
    "#Remove stop words\n",
    "stop_words = ['a','and','the','is']\n",
    "filtered_txt = shakespeare_rdd.flatMap(lambda x: x.split()).filter(lambda x: x not in stop_words)\n",
    "filtered_txt.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Stemming. Recognizing that various different words share the same root (\"run\", \"running\") is important, but not so\n",
    "# easy to do simply. Once again, Spark brings powerful libraries into the mix to help. A popular one is the Natural\n",
    "# Language Tool Kit. You should look at the docs, but you can give it a quick test quite easily:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE\n",
    "# Compute the average word length of shakespeare.txt.\n",
    "lengths_rdd = shakespeare_clean_rdd.flatMap(lambda line: [len(w) for w in line.split()])\n",
    "print(\"The average word length is {:.2f}\".format(lengths_rdd.sum() / lengths_rdd.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shakespeare_clean_rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21824/200120297.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# EXERCISE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Compute the average wordlenght for each letter of the alphabet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlength_to_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshakespeare_clean_rdd\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'shakespeare_clean_rdd' is not defined"
     ]
    }
   ],
   "source": [
    "# EXERCISE\n",
    "# Compute the average wordlenght for each letter of the alphabet\n",
    "avg_len_by_letter = shakespeare_clean\\\n",
    "    .flatMap(lambda line: [(w.lower()[0], len(w)) for w in line.split()])\\\n",
    "    .groupByKey()\\\n",
    "    .map(lambda x : (x[0], sum(list(x[1]))/len(list(x[1]))))\\\\\n",
    "    .collect()\n",
    "print(dict(avg_len_by_letter))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(\n",
    "    x=[letter for letter, avg in avg_len_by_letter],\n",
    "    height=[avg for letter, avg in avg_len_by_letter]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Compute the word length distribution of shakespeare.txt.\n",
    "length_to_count = shakespeare_clean_rdd\\\n",
    "    .flatMap(lambda line: [(len(w), 1) for w in line.split()])\\\n",
    "    .reduceByKey(lambda l, r: l+r)\\\n",
    "    .collect()\n",
    "print(dict(length_to_count))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(\n",
    "    x=[length for length, count in length_to_count],\n",
    "    height=[count for length, count in length_to_count]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "#Find the most frequent words of shakespeare.txt.\n",
    "shakespeare_clean_rdd\\\n",
    "    .flatMap(lambda line: [(w, 1) for w in line.split()])\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .takeOrdered(10, (lambda pair: -pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "#Compute term frequency (TF)\n",
    "shakespeare_clean_rdd\\\n",
    "    .flatMap(lambda line: [(w, 1) for w in line.split()])\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .takeOrdered(10, (lambda pair: -pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "# Count the number of words of shakespeare.txt starting with each letter of the alphabet.\n",
    "letters_to_count = shakespeare_clean_rdd\\\n",
    "    .flatMap(lambda line: [(w[0], 1) for w in line.split()])\\\n",
    "    .reduceByKey(lambda x,y: x+y)\\\n",
    "    .sortByKey()\\\n",
    "    .collect()\n",
    "print(dict(letters_to_count))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(\n",
    "    x=[l for l, freq in letters_to_count],\n",
    "    height=[freq for l, freq in letters_to_count]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Find the words pair most co-occurring next to each other of shakespeare.txt\n",
    "def line_to_bigram(line):\n",
    "    words = line.split()\n",
    "    return [(words[i-1], words[i]) for i in range(1, len(words))]\n",
    "\n",
    "shakespeare_clean_rdd\\\n",
    "    .flatMap(line_to_bigram)\\\n",
    "    .map(lambda bigram: (bigram, 1))\\\n",
    "    .reduceByKey(lambda x,y: x+y)\\\n",
    "    .takeOrdered(10, (lambda pair: -pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "#Compute term frequency (TF)\n",
    "tot_word = shakespeare_clean.flatMap(lambda line: line.split()).count()\n",
    "print(tot_word)\n",
    "shakespeare_clean\\\n",
    "    .flatMap(lambda line: line.split())\\\n",
    "    .filter(lambda w: w not in stop_words)\\\n",
    "    .map(lambda w: (w, 1))\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .mapValues(lambda x: x/tot_word)\\\n",
    "    .takeOrdered(20, (lambda pair: -pair[1]))\n",
    "\n",
    "# Compute IDF????\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "text_rdd = sc.textFile(\"/content/drive/MyDrive/input.txt\")\n",
    "\n",
    "# Filter the lines containing the word \"google\"\n",
    "google = text_rdd.filter(lambda l: \"google\" in l)\n",
    "google.collect()\n",
    "\n",
    "# Output: the list of distinct IP addresses associated with the connections to a googlepage (i.e., connections to URLs containing the term “www.google.com”)\n",
    "google = text_rdd.filter(lambda l: \"google\" in l).map(lambda line: line.split(\" --\")[0]).distinct()\n",
    "\n",
    "# Output: report the maximum value of PM10\n",
    "def split_and_get_temp(string):\n",
    "    l = string.split(',')\n",
    "    temp = l[2]\n",
    "    return float(temp)\n",
    "max_temp = text_rdd.map(split_and_get_temp).reduce(lambda t1,t2:t1 if t1>t2 else t2)\n",
    "max_temp\n",
    "\n",
    "# Output: the line(s) associated with the maximum value of PM10\n",
    "def split_and_get_temp(string):\n",
    "    l = string.split(',')\n",
    "    temp = l[2]\n",
    "    return float(temp)\n",
    "\n",
    "text_rdd = sc.textFile(\"/content/drive/MyDrive/input.txt\")\n",
    "max_temp = text_rdd.map(split_and_get_temp).top(1)[0]\n",
    "max_temp_vals = text_rdd.filter(lambda x : split_and_get_temp(x)==max_temp)\n",
    "max_temp_vals.collect()\n",
    "\n",
    "# Output: the date(s) associated with the maximum value of PM10\n",
    "max_temp_vals = text_rdd.filter(lambda x : split_and_get_temp(x)==max_temp).map(lambda s: s.split(',')[1])\n",
    "max_temp_vals.collect()\n",
    "\n",
    "# Output: the maximum value of PM10 for each sensor\n",
    "print(text_rdd.collect())\n",
    "max_temp_by_sensor = text_rdd.map(lambda x: (x.split(',')[0], x.split(',')[2])).reduceByKey(lambda x, y: max(x, y))\n",
    "max_temp_by_sensor = max_temp_by_sensor.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rdd = sc.parallelize([(1,2), (1,2), (1,3), (2,2), (2,4), (2,6)])\n",
    "#How can you obtain the output?\n",
    "# Sample output\n",
    "# 1,2,2,3\n",
    "# 2,2,4,6\n",
    "\n",
    "input_rdd = input_rdd.groupByKey().mapValues(list).sortByKey().map(lambda t: str(t[0])+','+(','.join([str(v) for v in t[1]])))\n",
    "output = '\\n'.join(input_rdd.collect())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Create an RDD with integers from 1-50. \n",
    "# Apply a transformation to multiply every number by 2, \n",
    "# resulting in an RDD that contains the first 50 even numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE\n",
    "# Word count problem\n",
    "# input : 2 textual files\n",
    "# output : number of occurrences of each word\n",
    "# appearing in at least one file of the collection (i.e.,\n",
    "# files of the input directory) and number of occurences of each word \n",
    "# appearing in both files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
